# Living Textbook Section: The History and Development of Artificial Intelligence and Large Language Models

### Date: May 2, 2025

### Course: CIDS 120

---

### Overview

In this lecture, we explored the conceptual, historical, and technical roots of artificial intelligence (AI), with a focus on large language models (LLMs) like ChatGPT. We discussed how ideas about machine intelligence have evolved over the past century and how those ideas culminated in the transformer-based LLMs we see today. We also touched on ethical concerns and posed questions about what it means for a machine to be "intelligent."

---

### 1. Defining Artificial Intelligence

AI is often portrayed as a futuristic concept, but its roots stretch back nearly a century. The term "artificial intelligence" was coined in 1956 at the Dartmouth Conference, but foundational ideas go back further, especially to the work of Alan Turing.

---

### 2. Alan Turing and the Origins of Computation

* **Turing Machine**: Proposed by Alan Turing in the 1930s, the Turing Machine is a hypothetical device that can perform any computable task. Turing called it the "universal machine," and it laid the groundwork for modern computing.

* **Turing Test**: In his 1950 paper *Computing Machinery and Intelligence*, Turing posed the question: "Can machines think?" He proposed the Turing Test, where a machine is deemed intelligent if it can carry on a conversation indistinguishable from a human.

---

### 3. The AI Winters and Early Experiments

* **Eliza (1966)**: A rule-based chatbot that simulated a Rogerian psychotherapist. It used pattern matching but lacked genuine understanding.

* **Expert Systems (1980s)**: AI programs designed for narrow domains like medicine or finance. These systems were brittle and could not learn or generalize.

* **AI Winters**: Periods of reduced funding and interest due to overpromises and underperformance. Notably, from 1974–1980 and again in the late 1980s to early 1990s.

---

### 4. Rise of Machine Learning (1990s–2010s)

* **Backpropagation (1986)**: Enabled training of multi-layer neural networks (aka perceptrons), leading to more sophisticated models.

* **Deep Blue (1997)**: IBM's computer defeated chess grandmaster Garry Kasparov. It was a milestone, but relied on brute force rather than true learning.

* **Statistical Learning**: Focus shifted to probabilistic models such as decision trees and Naive Bayes. These models approximated human-like decision-making more effectively than rule-based systems.

---

### 5. Data Science and Deep Learning (2000s)

* The explosion of available data and the use of GPUs enabled deep learning breakthroughs.
* **AlexNet (2012)**: A deep convolutional neural network that crushed benchmarks in image recognition tasks.

---

### 6. The Transformer Revolution (2017–Present)

* **"Attention Is All You Need" (2017)**: This paper introduced the Transformer architecture, which replaced recurrent models and became the foundation for LLMs.

* **GPT Series**:

    * **GPT (2018)**: Used unsupervised learning to analyze large text corpora.
    * **GPT-2 (2019)**: Notable for its fluency, initially withheld due to concerns over misuse.
    * **GPT-3 (2020)**: Trained on 175 billion parameters; showcased significant conversational capabilities.
    * **Codex (2021)**: Special version of GPT-3 fine-tuned for code generation.
    * **ChatGPT (2022)**: Fine-tuned GPT-3.5 for dialogue using reinforcement learning from human feedback (RLHF).
    * **GPT-4 (2023)**: Introduced multimodal capabilities (text + image inputs), better reasoning, and performance.

---

### 7. Supervised vs. Unsupervised Learning

* **Supervised Learning**: The model is trained on labeled examples (e.g., many images labeled as "cat").
* **Unsupervised Learning**: The model infers patterns from unlabeled data (e.g., finding common features without being told the labels).

---

### 8. Ethical and Philosophical Questions

* **What is Intelligence?**: If LLMs pass the Turing Test, does that mean they are intelligent? Do they require consciousness?
* **Elon Musk's Bootloader Theory**: Musk suggested that humans may be biological bootloaders for a future AI superintelligence.
* **Existential Risk**: The inability to fully understand how neural networks make decisions contributes to concerns about AI's potential danger.

---

### 9. Where We're Headed

* **Multimodal Models**: Newer LLMs can handle text, image, and video inputs.
* **Applications**: AI is increasingly used in education, healthcare, creative writing, and scientific research.
* **Competitors**: Other players include Meta's LLaMA, Anthropic's Claude, and Google's Gemini.

---

### 10. Challenges and Considerations

* **Bias & Hallucinations**
* **Data Privacy & Provenance**
* **Job Displacement vs. Augmentation**
* **Need for Regulation, Transparency, and Alignment**

---

### Reflection Prompt

As you reflect on today’s lecture, consider:

* Do you think LLMs like ChatGPT are intelligent by your own definition?
* Should society regulate AI the same way we regulate other powerful technologies?

Be prepared to discuss this on Monday.

---

### Reminders

* Your **anti-position paper** is due **Monday**.
* The **final quiz** is due **next Friday**.

Please email me with any questions. Have a great weekend!

---
